{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Synthesizing high-quality images from text descriptions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TnczxE21oTqw"
   },
   "source": [
    "## Implementation of \"Stage 1 \" of **StackGAN**\n",
    "\n",
    "### Stage I of StackGAN \n",
    "\n",
    "#### 1- takes input as text, \n",
    "\n",
    "#### 2- convert the text to embedding using our pre-trained character level embedding. \n",
    "\n",
    "#### 3- Then, we give this embedding to Conditional Augmentation (CA) and \n",
    "\n",
    "#### 4- then to Stage I Generator which gives us low-resolution 64*64 images. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "StpwZzOYxFRo"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\purpl\\anaconda3\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"StackGAN.\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "assert tf.__version__.startswith('2')\n",
    "\n",
    "import PIL\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras import Input, Model\n",
    "from tensorflow.keras.layers import LeakyReLU, BatchNormalization, ReLU, Activation\n",
    "from tensorflow.keras.layers import UpSampling2D, Conv2D, Concatenate, Dense, concatenate\n",
    "from tensorflow.keras.layers import Flatten, Lambda, Reshape, ZeroPadding2D, add\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8Rb4IqOBxN1C"
   },
   "source": [
    "\n",
    "############################################################\n",
    "# Conditioning Augmentation Network\n",
    "############################################################\n",
    "\n",
    "\n",
    "Computer doesn’t understand words, but it can represent the words in terms of something it does “understand”. That’s the “text embedding”, and it’s used as the c\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "SO_uUHnyxNOU"
   },
   "outputs": [],
   "source": [
    "# conditioned by the text.\n",
    "def conditioning_augmentation(x):\n",
    "\t\"\"\"The mean_logsigma passed as argument is converted into the text conditioning variable.\n",
    "\n",
    "\tArgs:\n",
    "\t\tx: The output of the text embedding passed through a FC layer with LeakyReLU non-linearity.\n",
    "\n",
    "\tReturns:\n",
    "\t \tc: The text conditioning variable after computation.\n",
    "\t\"\"\"\n",
    "\tmean = x[:, :128]\n",
    "\tlog_sigma = x[:, 128:]\n",
    "\n",
    "\tstddev = tf.math.exp(log_sigma)\n",
    "\tepsilon = K.random_normal(shape=K.constant((mean.shape[1], ), dtype='int32'))\n",
    "\tc = mean + stddev * epsilon\n",
    "\treturn c\n",
    "\n",
    "def build_ca_network():\n",
    "\t\"\"\"Builds the conditioning augmentation network.\n",
    "\t\"\"\"\n",
    "\tinput_layer1 = Input(shape=(1024,)) #size of the vocabulary in the text data\n",
    "\tmls = Dense(256)(input_layer1)\n",
    "\tmls = LeakyReLU(alpha=0.2)(mls)\n",
    "\tca = Lambda(conditioning_augmentation)(mls)\n",
    "\treturn Model(inputs=[input_layer1], outputs=[ca]) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "############################################################\n",
    "# Stage 1 Generator Network \n",
    "############################################################\n",
    "\n",
    "1. The generator is fed with the text captions in the form of Embedding vectors which will be used to condition its generation of features.\n",
    "2. A vector with random noise.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "8BaEBiaCxFUd"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def UpSamplingBlock(x, num_kernels):\n",
    "\t\"\"\"An Upsample block with Upsampling2D, Conv2D, BatchNormalization and a ReLU activation.\n",
    "\n",
    "\tArgs:\n",
    "\t\tx: The preceding layer as input.\n",
    "\t\tnum_kernels: Number of kernels for the Conv2D layer.\n",
    "\n",
    "\tReturns:\n",
    "\t\tx: The final activation layer after the Upsampling block.\n",
    "\t\"\"\"\n",
    "\tx = UpSampling2D(size=(2,2))(x)\n",
    "\tx = Conv2D(num_kernels, kernel_size=(3,3), padding='same', strides=1, use_bias=False,\n",
    "\t\t\t\tkernel_initializer='he_uniform')(x)\n",
    "\tx = BatchNormalization(gamma_initializer='ones', beta_initializer='zeros')(x) #prevent from mode collapse\n",
    "\tx = ReLU()(x)\n",
    "\treturn x\n",
    "\n",
    "\n",
    "def build_stage1_generator():\n",
    "\n",
    "\tinput_layer1 = Input(shape=(1024,))\n",
    "\tca = Dense(256)(input_layer1)\n",
    "\tca = LeakyReLU(alpha=0.2)(ca)\n",
    "\n",
    "\t# Obtain the conditioned text\n",
    "\tc = Lambda(conditioning_augmentation)(ca)\n",
    "\n",
    "\tinput_layer2 = Input(shape=(100,))\n",
    "\tconcat = Concatenate(axis=1)([c, input_layer2]) \n",
    "\n",
    "\tx = Dense(16384, use_bias=False)(concat) \n",
    "\tx = ReLU()(x)\n",
    "\tx = Reshape((4, 4, 1024), input_shape=(16384,))(x)\n",
    "\n",
    "\tx = UpSamplingBlock(x, 512) \n",
    "\tx = UpSamplingBlock(x, 256)\n",
    "\tx = UpSamplingBlock(x, 128)\n",
    "\tx = UpSamplingBlock(x, 64)   # upsampled our image to 64*64*3 \n",
    "\n",
    "\tx = Conv2D(3, kernel_size=3, padding='same', strides=1, use_bias=False,\n",
    "\t\t\t\tkernel_initializer='he_uniform')(x)\n",
    "\tx = Activation('tanh')(x)\n",
    "\n",
    "\tstage1_gen = Model(inputs=[input_layer1, input_layer2], outputs=[x, ca]) \n",
    "\treturn stage1_gen\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\purpl\\anaconda3\\lib\\site-packages\\keras\\src\\backend.py:1398: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\purpl\\anaconda3\\lib\\site-packages\\keras\\src\\layers\\normalization\\batch_normalization.py:979: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
      "\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)        [(None, 1024)]               0         []                            \n",
      "                                                                                                  \n",
      " dense (Dense)               (None, 256)                  262400    ['input_1[0][0]']             \n",
      "                                                                                                  \n",
      " leaky_re_lu (LeakyReLU)     (None, 256)                  0         ['dense[0][0]']               \n",
      "                                                                                                  \n",
      " lambda (Lambda)             (None, 128)                  0         ['leaky_re_lu[0][0]']         \n",
      "                                                                                                  \n",
      " input_2 (InputLayer)        [(None, 100)]                0         []                            \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)   (None, 228)                  0         ['lambda[0][0]',              \n",
      "                                                                     'input_2[0][0]']             \n",
      "                                                                                                  \n",
      " dense_1 (Dense)             (None, 16384)                3735552   ['concatenate[0][0]']         \n",
      "                                                                                                  \n",
      " re_lu (ReLU)                (None, 16384)                0         ['dense_1[0][0]']             \n",
      "                                                                                                  \n",
      " reshape (Reshape)           (None, 4, 4, 1024)           0         ['re_lu[0][0]']               \n",
      "                                                                                                  \n",
      " up_sampling2d (UpSampling2  (None, 8, 8, 1024)           0         ['reshape[0][0]']             \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " conv2d (Conv2D)             (None, 8, 8, 512)            4718592   ['up_sampling2d[0][0]']       \n",
      "                                                                                                  \n",
      " batch_normalization (Batch  (None, 8, 8, 512)            2048      ['conv2d[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " re_lu_1 (ReLU)              (None, 8, 8, 512)            0         ['batch_normalization[0][0]'] \n",
      "                                                                                                  \n",
      " up_sampling2d_1 (UpSamplin  (None, 16, 16, 512)          0         ['re_lu_1[0][0]']             \n",
      " g2D)                                                                                             \n",
      "                                                                                                  \n",
      " conv2d_1 (Conv2D)           (None, 16, 16, 256)          1179648   ['up_sampling2d_1[0][0]']     \n",
      "                                                                                                  \n",
      " batch_normalization_1 (Bat  (None, 16, 16, 256)          1024      ['conv2d_1[0][0]']            \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " re_lu_2 (ReLU)              (None, 16, 16, 256)          0         ['batch_normalization_1[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " up_sampling2d_2 (UpSamplin  (None, 32, 32, 256)          0         ['re_lu_2[0][0]']             \n",
      " g2D)                                                                                             \n",
      "                                                                                                  \n",
      " conv2d_2 (Conv2D)           (None, 32, 32, 128)          294912    ['up_sampling2d_2[0][0]']     \n",
      "                                                                                                  \n",
      " batch_normalization_2 (Bat  (None, 32, 32, 128)          512       ['conv2d_2[0][0]']            \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " re_lu_3 (ReLU)              (None, 32, 32, 128)          0         ['batch_normalization_2[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " up_sampling2d_3 (UpSamplin  (None, 64, 64, 128)          0         ['re_lu_3[0][0]']             \n",
      " g2D)                                                                                             \n",
      "                                                                                                  \n",
      " conv2d_3 (Conv2D)           (None, 64, 64, 64)           73728     ['up_sampling2d_3[0][0]']     \n",
      "                                                                                                  \n",
      " batch_normalization_3 (Bat  (None, 64, 64, 64)           256       ['conv2d_3[0][0]']            \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " re_lu_4 (ReLU)              (None, 64, 64, 64)           0         ['batch_normalization_3[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " conv2d_4 (Conv2D)           (None, 64, 64, 3)            1728      ['re_lu_4[0][0]']             \n",
      "                                                                                                  \n",
      " activation (Activation)     (None, 64, 64, 3)            0         ['conv2d_4[0][0]']            \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 10270400 (39.18 MB)\n",
      "Trainable params: 10268480 (39.17 MB)\n",
      "Non-trainable params: 1920 (7.50 KB)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "generator = build_stage1_generator()\n",
    "generator.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bc7hLipAxcXb"
   },
   "source": [
    "\n",
    "############################################################\n",
    "# Stage 1 Discriminator Network\n",
    "############################################################\t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "JwHBP8sCxFXg"
   },
   "outputs": [],
   "source": [
    "def ConvBlock(x, num_kernels, kernel_size=(4,4), strides=2, activation=True):\n",
    "\t\"\"\"A ConvBlock with a Conv2D, BatchNormalization and LeakyReLU activation.\n",
    "\n",
    "\tArgs:\n",
    "\t\tx: The preceding layer as input.\n",
    "\t\tnum_kernels: Number of kernels for the Conv2D layer.\n",
    "\n",
    "\tReturns:\n",
    "\t\tx: The final activation layer after the ConvBlock block.\n",
    "\t\"\"\"\n",
    "\tx = Conv2D(num_kernels, kernel_size=kernel_size, padding='same', strides=strides, use_bias=False,\n",
    "\t\t\t\tkernel_initializer='he_uniform')(x)\n",
    "\tx = BatchNormalization(gamma_initializer='ones', beta_initializer='zeros')(x)\n",
    "\t\n",
    "\tif activation:\n",
    "\t\tx = LeakyReLU(alpha=0.2)(x)\n",
    "\treturn x\n",
    "\n",
    "\n",
    "def build_embedding_compressor():\n",
    "    \"\"\"Build embedding compressor model\n",
    "    \"\"\"\n",
    "    input_layer1 = Input(shape=(1024,)) \n",
    "    x = Dense(128)(input_layer1)\n",
    "    x = ReLU()(x)\n",
    "\n",
    "    model = Model(inputs=[input_layer1], outputs=[x])\n",
    "    return model\n",
    "\n",
    "# the discriminator is fed with two inputs, the feature from Generator and the text embedding\n",
    "def build_stage1_discriminator():\n",
    "\t\"\"\"Builds the Stage 1 Discriminator that uses the 64x64 resolution images from the generator\n",
    "\tand the compressed and spatially replicated embedding.\n",
    "\n",
    "\tReturns:\n",
    "\t\tStage 1 Discriminator Model for StackGAN.\n",
    "\t\"\"\"\n",
    "\tinput_layer1 = Input(shape=(64, 64, 3))  \n",
    "\n",
    "\tx = Conv2D(64, kernel_size=(4,4), strides=2, padding='same', use_bias=False,\n",
    "\t\t\t\tkernel_initializer='he_uniform')(input_layer1)\n",
    "\tx = LeakyReLU(alpha=0.2)(x)\n",
    "\n",
    "\tx = ConvBlock(x, 128)\n",
    "\tx = ConvBlock(x, 256)\n",
    "\tx = ConvBlock(x, 512)\n",
    "\n",
    "\t# Obtain the compressed and spatially replicated text embedding\n",
    "\tinput_layer2 = Input(shape=(4, 4, 128)) #2nd input to discriminator, text embedding\n",
    "\tconcat = concatenate([x, input_layer2])\n",
    "\n",
    "\tx1 = Conv2D(512, kernel_size=(1,1), padding='same', strides=1, use_bias=False,\n",
    "\t\t\t\tkernel_initializer='he_uniform')(concat)\n",
    "\tx1 = BatchNormalization(gamma_initializer='ones', beta_initializer='zeros')(x)\n",
    "\tx1 = LeakyReLU(alpha=0.2)(x)\n",
    "\n",
    "\t# Flatten and add a FC layer to predict.\n",
    "\tx1 = Flatten()(x1)\n",
    "\tx1 = Dense(1)(x1)\n",
    "\tx1 = Activation('sigmoid')(x1)\n",
    "\n",
    "\tstage1_dis = Model(inputs=[input_layer1, input_layer2], outputs=[x1])  \n",
    "\treturn stage1_dis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_3 (InputLayer)        [(None, 64, 64, 3)]          0         []                            \n",
      "                                                                                                  \n",
      " conv2d_5 (Conv2D)           (None, 32, 32, 64)           3072      ['input_3[0][0]']             \n",
      "                                                                                                  \n",
      " leaky_re_lu_1 (LeakyReLU)   (None, 32, 32, 64)           0         ['conv2d_5[0][0]']            \n",
      "                                                                                                  \n",
      " conv2d_6 (Conv2D)           (None, 16, 16, 128)          131072    ['leaky_re_lu_1[0][0]']       \n",
      "                                                                                                  \n",
      " batch_normalization_4 (Bat  (None, 16, 16, 128)          512       ['conv2d_6[0][0]']            \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " leaky_re_lu_2 (LeakyReLU)   (None, 16, 16, 128)          0         ['batch_normalization_4[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " conv2d_7 (Conv2D)           (None, 8, 8, 256)            524288    ['leaky_re_lu_2[0][0]']       \n",
      "                                                                                                  \n",
      " batch_normalization_5 (Bat  (None, 8, 8, 256)            1024      ['conv2d_7[0][0]']            \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " leaky_re_lu_3 (LeakyReLU)   (None, 8, 8, 256)            0         ['batch_normalization_5[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " conv2d_8 (Conv2D)           (None, 4, 4, 512)            2097152   ['leaky_re_lu_3[0][0]']       \n",
      "                                                                                                  \n",
      " batch_normalization_6 (Bat  (None, 4, 4, 512)            2048      ['conv2d_8[0][0]']            \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " leaky_re_lu_4 (LeakyReLU)   (None, 4, 4, 512)            0         ['batch_normalization_6[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " leaky_re_lu_5 (LeakyReLU)   (None, 4, 4, 512)            0         ['leaky_re_lu_4[0][0]']       \n",
      "                                                                                                  \n",
      " flatten (Flatten)           (None, 8192)                 0         ['leaky_re_lu_5[0][0]']       \n",
      "                                                                                                  \n",
      " dense_2 (Dense)             (None, 1)                    8193      ['flatten[0][0]']             \n",
      "                                                                                                  \n",
      " input_4 (InputLayer)        [(None, 4, 4, 128)]          0         []                            \n",
      "                                                                                                  \n",
      " activation_1 (Activation)   (None, 1)                    0         ['dense_2[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2767361 (10.56 MB)\n",
      "Trainable params: 2765569 (10.55 MB)\n",
      "Non-trainable params: 1792 (7.00 KB)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "discriminator = build_stage1_discriminator()\n",
    "discriminator.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SXNLnmkFxjI0"
   },
   "source": [
    "\n",
    "############################################################\n",
    "# Stage 1 Adversarial Model  (Building a GAN)\n",
    "############################################################\n",
    "\n",
    "Generator and discriminator are stacked together. Output of the former is the input of the latter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "8rjVyqYhxFag"
   },
   "outputs": [],
   "source": [
    "# Building GAN with Generator and Discriminator\n",
    "\n",
    "def build_adversarial(generator_model, discriminator_model):\n",
    "\t\"\"\"Stage 1 Adversarial model.\n",
    "\n",
    "\tArgs:\n",
    "\t\tgenerator_model: Stage 1 Generator Model\n",
    "\t\tdiscriminator_model: Stage 1 Discriminator Model\n",
    "\n",
    "\tReturns:\n",
    "\t\tAdversarial Model.\n",
    "\t\"\"\"\n",
    "\tinput_layer1 = Input(shape=(1024,))  \n",
    "\tinput_layer2 = Input(shape=(100,)) \n",
    "\tinput_layer3 = Input(shape=(4, 4, 128)) \n",
    "\n",
    "\tx, ca = generator_model([input_layer1, input_layer2]) #text,noise\n",
    "\n",
    "\tdiscriminator_model.trainable = False \n",
    "\n",
    "\tprobabilities = discriminator_model([x, input_layer3]) \n",
    "\tadversarial_model = Model(inputs=[input_layer1, input_layer2, input_layer3], outputs=[probabilities, ca])\n",
    "\treturn adversarial_model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_5 (InputLayer)        [(None, 1024)]               0         []                            \n",
      "                                                                                                  \n",
      " input_6 (InputLayer)        [(None, 100)]                0         []                            \n",
      "                                                                                                  \n",
      " model (Functional)          [(None, 64, 64, 3),          1027040   ['input_5[0][0]',             \n",
      "                              (None, 256)]                0          'input_6[0][0]']             \n",
      "                                                                                                  \n",
      " input_7 (InputLayer)        [(None, 4, 4, 128)]          0         []                            \n",
      "                                                                                                  \n",
      " model_1 (Functional)        (None, 1)                    2767361   ['model[0][0]',               \n",
      "                                                                     'input_7[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 13037761 (49.74 MB)\n",
      "Trainable params: 10268480 (39.17 MB)\n",
      "Non-trainable params: 2769281 (10.56 MB)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "ganstage1 = build_adversarial(generator, discriminator)\n",
    "ganstage1.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g4Dy_RuO42Am"
   },
   "source": [
    "############################################################\n",
    "# Train Utilities\n",
    "############################################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "tuXx7HDj41XW"
   },
   "outputs": [],
   "source": [
    "\n",
    "def checkpoint_prefix():\n",
    "\tcheckpoint_dir = './training_checkpoints'\n",
    "\tcheckpoint_prefix = os.path.join(checkpoint_dir, 'ckpt')\n",
    "\n",
    "\treturn checkpoint_prefix\n",
    "\n",
    "def adversarial_loss(y_true, y_pred):\n",
    "\tmean = y_pred[:, :128]\n",
    "\tls = y_pred[:, 128:]\n",
    "\tloss = -ls + 0.5 * (-1 + tf.math.exp(2.0 * ls) + tf.math.square(mean))\n",
    "\tloss = K.mean(loss)\n",
    "\treturn loss\n",
    "\n",
    "def normalize(input_image, real_image):\n",
    "\tinput_image = (input_image / 127.5) - 1\n",
    "\treal_image = (real_image / 127.5) - 1\n",
    "\n",
    "\treturn input_image, real_image\n",
    "\n",
    "def load_class_ids_filenames(class_id_path, filename_path):\n",
    "\twith open(class_id_path, 'rb') as file:\n",
    "\t\tclass_id = pickle.load(file, encoding='latin1')\n",
    "\n",
    "\twith open(filename_path, 'rb') as file:\n",
    "\t\tfilename = pickle.load(file, encoding='latin1')\n",
    "\n",
    "\treturn class_id, filename\n",
    "\n",
    "def load_text_embeddings(text_embeddings):\n",
    "\twith open(text_embeddings, 'rb') as file:\n",
    "\t\tembeds = pickle.load(file, encoding='latin1')\n",
    "\t\tembeds = np.array(embeds)\n",
    "\n",
    "\treturn embeds\n",
    "\n",
    "def load_bbox(data_path):\n",
    "\tbbox_path = data_path + '/bounding_boxes.txt'\n",
    "\timage_path = data_path + '/images.txt'\n",
    "\tbbox_df = pd.read_csv(bbox_path, delim_whitespace=True, header=None).astype(int)\n",
    "\tfilename_df = pd.read_csv(image_path, delim_whitespace=True, header=None)\n",
    "\n",
    "\tfilenames = filename_df[1].tolist()\n",
    "\tbbox_dict = {i[:-4]:[] for i in filenames[:2]}\n",
    "\n",
    "\tfor i in range(0, len(filenames)):\n",
    "\t\tbbox = bbox_df.iloc[i][1:].tolist()\n",
    "\t\tdict_key = filenames[i][:-4]\n",
    "\t\tbbox_dict[dict_key] = bbox\n",
    "\n",
    "\treturn bbox_dict\n",
    "\n",
    "def load_images(image_path, bounding_box, size):\n",
    "\t\"\"\"Crops the image to the bounding box and then resizes it.\n",
    "\t\"\"\"\n",
    "\timage = Image.open(image_path).convert('RGB')\n",
    "\tw, h = image.size\n",
    "\tif bounding_box is not None:\n",
    "\t\tr = int(np.maximum(bounding_box[2], bounding_box[3]) * 0.75)\n",
    "\t\tc_x = int((bounding_box[0] + bounding_box[2]) / 2)\n",
    "\t\tc_y = int((bounding_box[1] + bounding_box[3]) / 2)\n",
    "\t\ty1 = np.maximum(0, c_y - r)\n",
    "\t\ty2 = np.minimum(h, c_y + r)\n",
    "\t\tx1 = np.maximum(0, c_x - r)\n",
    "\t\tx2 = np.minimum(w, c_x + r)\n",
    "\t\timage = image.crop([x1, y1, x2, y2])\n",
    "\n",
    "\timage = image.resize(size, PIL.Image.BILINEAR)\n",
    "\treturn image\n",
    "\n",
    "def load_data(filename_path, class_id_path, dataset_path, embeddings_path, size):\n",
    "\t\"\"\"Loads the Dataset.\n",
    "\t\"\"\"\n",
    "\tdata_dir = \"C:/Users/purpl/Downloads/text_to_image_GAN/birds\"\n",
    "\ttrain_dir = data_dir + \"/train\"\n",
    "\ttest_dir = data_dir + \"/test\"\n",
    "\tembeddings_path_train = train_dir + \"/char-CNN-RNN-embeddings.pickle\"\n",
    "\tembeddings_path_test = test_dir + \"/char-CNN-RNN-embeddings.pickle\"\n",
    "\tfilename_path_train = train_dir + \"/filenames.pickle\"\n",
    "\tfilename_path_test = test_dir + \"/filenames.pickle\"\n",
    "\tclass_id_path_train = train_dir + \"/class_info.pickle\"\n",
    "\tclass_id_path_test = test_dir + \"/class_info.pickle\"\n",
    "\tdataset_path = \"C:/Users/purpl/Downloads/text_to_image_GAN/CUB_200_2011/CUB_200_2011\"\n",
    "\tclass_id, filenames = load_class_ids_filenames(class_id_path, filename_path)\n",
    "\tembeddings = load_text_embeddings(embeddings_path)\n",
    "\tbbox_dict = load_bbox(dataset_path)\n",
    "\n",
    "\tx, y, embeds = [], [], []\n",
    "\n",
    "\tfor i, filename in enumerate(filenames):\n",
    "\t\tbbox = bbox_dict[filename]\n",
    "\n",
    "\t\ttry:\t\n",
    "\t\t\timage_path = f'{dataset_path}/images/{filename}.jpg'\n",
    "\t\t\timage = load_images(image_path, bbox, size)\n",
    "\t\t\te = embeddings[i, :, :]\n",
    "\t\t\tembed_index = np.random.randint(0, e.shape[0] - 1)\n",
    "\t\t\tembed = e[embed_index, :]\n",
    "\n",
    "\t\t\tx.append(np.array(image))\n",
    "\t\t\ty.append(class_id[i])\n",
    "\t\t\tembeds.append(embed)\n",
    "\n",
    "\t\texcept Exception as e:\n",
    "\t\t\tprint(f'{e}')\n",
    "\t\n",
    "\tx = np.array(x)\n",
    "\ty = np.array(y)\n",
    "\tembeds = np.array(embeds)\n",
    "\t\n",
    "\treturn x, y, embeds\n",
    "\n",
    "def save_image(file, save_path):\n",
    "\t\"\"\"Saves the image at the specified file path.\n",
    "\t\"\"\"\n",
    "\timage = plt.figure()\n",
    "\tax = image.add_subplot(1,1,1)\n",
    "\tax.imshow(file)\n",
    "\tax.axis(\"off\")\n",
    "\tplt.savefig(save_path)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "pjcH1V8ax9A1"
   },
   "outputs": [],
   "source": [
    "\n",
    "############################################################\n",
    "# StackGAN class\n",
    "############################################################\n",
    "\n",
    "class StackGanStage1(object):\n",
    "    \"\"\"StackGAN Stage 1 class.\"\"\"\n",
    "\n",
    "    data_dir = r'C:/Users/purpl/Downloads/text_to_image_GAN/birds'\n",
    "    train_dir = data_dir + \"/train\"\n",
    "    test_dir = data_dir + \"/test\"\n",
    "    embeddings_path_train = train_dir + \"/char-CNN-RNN-embeddings.pickle\"\n",
    "    embeddings_path_test = test_dir + \"/char-CNN-RNN-embeddings.pickle\"\n",
    "    filename_path_train = train_dir + \"/filenames.pickle\"\n",
    "    filename_path_test = test_dir + \"/filenames.pickle\"\n",
    "    class_id_path_train = train_dir + \"/class_info.pickle\"\n",
    "    class_id_path_test = test_dir + \"/class_info.pickle\"\n",
    "    dataset_path = r'C:/Users/purpl/Downloads/text_to_image_GAN/CUB_200_2011/CUB_200_2011'\n",
    "    def __init__(self, epochs=500, z_dim=100, batch_size=64, enable_function=True, stage1_generator_lr=0.0002, stage1_discriminator_lr=0.0002):\n",
    "        self.epochs = epochs\n",
    "        self.z_dim = z_dim\n",
    "        self.enable_function = enable_function\n",
    "        self.stage1_generator_lr = stage1_generator_lr\n",
    "        self.stage1_discriminator_lr = stage1_discriminator_lr\n",
    "        self.image_size = 64\n",
    "        self.conditioning_dim = 128\n",
    "        self.batch_size = batch_size\n",
    "      \n",
    "        self.stage1_generator_optimizer = Adam(lr=stage1_generator_lr, beta_1=0.5, beta_2=0.999)\n",
    "        self.stage1_discriminator_optimizer = Adam(lr=stage1_discriminator_lr, beta_1=0.5, beta_2=0.999)\n",
    "      \n",
    "        self.stage1_generator = build_stage1_generator()\n",
    "        self.stage1_generator.compile(loss='mse', optimizer=self.stage1_generator_optimizer)\n",
    "        self.stage1_discriminator = build_stage1_discriminator()\n",
    "        self.stage1_discriminator.compile(loss='binary_crossentropy', optimizer=self.stage1_discriminator_optimizer)\n",
    "        self.ca_network = build_ca_network()\n",
    "        self.ca_network.compile(loss='binary_crossentropy', optimizer='Adam')\n",
    "        self.embedding_compressor = build_embedding_compressor()\n",
    "        self.embedding_compressor.compile(loss='binary_crossentropy', optimizer='Adam')\n",
    "        self.stage1_adversarial = build_adversarial(self.stage1_generator, self.stage1_discriminator)\n",
    "        self.stage1_adversarial.compile(loss=['binary_crossentropy', adversarial_loss], loss_weights=[1, 2.0], optimizer=self.stage1_generator_optimizer)\n",
    "        self.checkpoint1 = tf.train.Checkpoint(\n",
    "        \tgenerator_optimizer=self.stage1_generator_optimizer,\n",
    "        \tdiscriminator_optimizer=self.stage1_discriminator_optimizer,\n",
    "        \tgenerator=self.stage1_generator,\n",
    "        \tdiscriminator=self.stage1_discriminator)\n",
    "\n",
    "    def visualize_stage1(self):\n",
    "        \"\"\"Running Tensorboard visualizations.\"\"\"\n",
    "        tb = TensorBoard(log_dir=\"logs/\".format(time.time()))\n",
    "        tb.set_model(self.stage1_generator)\n",
    "        tb.set_model(self.stage1_discriminator)\n",
    "        tb.set_model(self.ca_network)\n",
    "        tb.set_model(self.embedding_compressor)\n",
    "\n",
    "    def train_stage1(self):\n",
    "        \"\"\"Trains the stage1 StackGAN.\"\"\"\n",
    "        x_train, y_train, train_embeds = load_data(filename_path=self.filename_path_train, class_id_path=self.class_id_path_train, dataset_path=self.dataset_path, embeddings_path=self.embeddings_path_train, size=(64, 64))\n",
    "\n",
    "        x_test, y_test, test_embeds = load_data(filename_path=self.filename_path_test, class_id_path=self.class_id_path_test, dataset_path=self.dataset_path, embeddings_path=self.embeddings_path_test, size=(64, 64))\n",
    "\n",
    "        real = np.ones((self.batch_size, 1), dtype='float') * 0.9\n",
    "        fake = np.zeros((self.batch_size, 1), dtype='float') * 0.1\n",
    "\n",
    "        for epoch in range(self.epochs):\n",
    "            print(f'Epoch: {epoch}')\n",
    "            gen_loss = []\n",
    "            dis_loss = []\n",
    "\n",
    "            num_batches = int(x_train.shape[0] / self.batch_size)\n",
    "\n",
    "            for i in range(num_batches):\n",
    "\n",
    "                latent_space = np.random.normal(0, 1, size=(self.batch_size, self.z_dim))\n",
    "                embedding_text = train_embeds[i * self.batch_size:(i + 1) * self.batch_size]\n",
    "                compressed_embedding = self.embedding_compressor.predict_on_batch(embedding_text)\n",
    "                compressed_embedding = np.reshape(compressed_embedding, (-1, 1, 1, 128))\n",
    "                compressed_embedding = np.tile(compressed_embedding, (1, 4, 4, 1))\n",
    "\n",
    "                image_batch = x_train[i * self.batch_size:(i+1) * self.batch_size]\n",
    "                image_batch = (image_batch - 127.5) / 127.5\n",
    "\n",
    "                gen_images, _ = self.stage1_generator.predict([embedding_text, latent_space])\n",
    "\n",
    "                discriminator_loss = self.stage1_discriminator.train_on_batch([image_batch, compressed_embedding], np.reshape(real, (self.batch_size, 1)))\n",
    "\n",
    "                discriminator_loss_gen = self.stage1_discriminator.train_on_batch([gen_images, compressed_embedding], np.reshape(fake, (self.batch_size, 1)))\n",
    "\n",
    "                discriminator_loss_wrong = self.stage1_discriminator.train_on_batch([gen_images[: self.batch_size-1], compressed_embedding[1:]], np.reshape(fake[1:], (self.batch_size-1, 1)))\n",
    "\n",
    "                # Discriminator loss\n",
    "                d_loss = 0.5 * np.add(discriminator_loss, 0.5 * np.add(discriminator_loss_gen, discriminator_loss_wrong))\n",
    "                dis_loss.append(d_loss)\n",
    "                print(f'Discriminator Loss: {d_loss}')\n",
    "\n",
    "                # Generator loss\n",
    "                g_loss = self.stage1_adversarial.train_on_batch([embedding_text, latent_space, compressed_embedding],[K.ones((self.batch_size, 1)) * 0.9, K.ones((self.batch_size, 256)) * 0.9])\n",
    "\n",
    "                print(f'Generator Loss: {g_loss}')\n",
    "                gen_loss.append(g_loss)\n",
    "\n",
    "                if epoch % 5 == 0:\n",
    "                    latent_space = np.random.normal(0, 1, size=(self.batch_size, self.z_dim))\n",
    "                    embedding_batch = test_embeds[0 : self.batch_size]\n",
    "                    gen_images, _ = self.stage1_generator.predict_on_batch([embedding_batch, latent_space])\n",
    "\n",
    "                    for i, image in enumerate(gen_images[:10]):\n",
    "                        save_image(image, f'C:/Users/purpl/Downloads/text_to_image_GAN/test/gen_1_{epoch}_{i}.png')\n",
    "\n",
    "                if epoch % 25 == 0:\n",
    "                    self.stage1_generator.save_weights('C:/Users/purpl/Downloads/text_to_image_GAN/weights/stage1_gen.h5')\n",
    "                    self.stage1_discriminator.save_weights(\"C:/Users/purpl/Downloads/text_to_image_GAN/weights/stage1_disc.h5\")\n",
    "                    self.ca_network.save_weights('C:/Users/purpl/Downloads/text_to_image_GAN/weights/stage1_ca.h5')\n",
    "                    self.embedding_compressor.save_weights('C:/Users/purpl/Downloads/text_to_image_GAN/weights/stage1_embco.h5')\n",
    "                    self.stage1_adversarial.save_weights('C:/Users/purpl/Downloads/text_to_image_GAN/weights/stage1_adv.h5')      \n",
    "\n",
    "        self.stage1_generator.save_weights('C:/Users/purpl/Downloads/text_to_image_GAN/weights/stage1_gen.h5')\n",
    "        self.stage1_discriminator.save_weights(\"C:/Users/purpl/Downloads/text_to_image_GAN/weights/stage1_disc.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WUDlLFK0xFdV",
    "outputId": "3aec9e0e-cff9-4742-a2d3-ce4161be794e"
   },
   "outputs": [],
   "source": [
    "stage1 = StackGanStage1()\n",
    "stage1.train_stage1()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "inj5IwROxFgE"
   },
   "source": [
    "## Check test folder for generated images from Stage1 Generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's Implement Stage 2 Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################\n",
    "# Stage 2 Generator Network\n",
    "############################################################\n",
    "\n",
    "def concat_along_dims(inputs):\n",
    "\t\"\"\"Joins the conditioned text with the encoded image along the dimensions.\n",
    "\n",
    "\tArgs:\n",
    "\t\tinputs: consisting of conditioned text and encoded images as [c,x].\n",
    "\n",
    "\tReturns:\n",
    "\t\tJoint block along the dimensions.\n",
    "\t\"\"\"\n",
    "\tc = inputs[0]\n",
    "\tx = inputs[1]\n",
    "\n",
    "\tc = K.expand_dims(c, axis=1)\n",
    "\tc = K.expand_dims(c, axis=1)\n",
    "\tc = K.tile(c, [1, 16, 16, 1])\n",
    "\treturn K.concatenate([c, x], axis = 3)\n",
    "\n",
    "def residual_block(input):\n",
    "\t\"\"\"Residual block with plain identity connections.\n",
    "\n",
    "\tArgs:\n",
    "\t\tinputs: input layer or an encoded layer\n",
    "\n",
    "\tReturns:\n",
    "\t\tLayer with computed identity mapping.\n",
    "\t\"\"\"\n",
    "\tx = Conv2D(512, kernel_size=(3,3), padding='same', use_bias=False,\n",
    "\t\t\t\tkernel_initializer='he_uniform')(input)\n",
    "\tx = BatchNormalization(gamma_initializer='ones', beta_initializer='zeros')(x)\n",
    "\tx = ReLU()(x)\n",
    "\t\n",
    "\tx = Conv2D(512, kernel_size=(3,3), padding='same', use_bias=False,\n",
    "\t\t\t\tkernel_initializer='he_uniform')(x)\n",
    "\tx = BatchNormalization(gamma_initializer='ones', beta_initializer='zeros')(x)\n",
    "\t\n",
    "\tx = add([x, input])\n",
    "\tx = ReLU()(x)\n",
    "\n",
    "\treturn x\n",
    "\n",
    "def build_stage2_generator():\n",
    "\t\"\"\"Build the Stage 2 Generator Network using the conditioning text and images from stage 1.\n",
    "\n",
    "\tReturns:\n",
    "\t\tStage 2 Generator Model for StackGAN.\n",
    "\t\"\"\"\n",
    "\tinput_layer1 = Input(shape=(1024,))\n",
    "\tinput_images = Input(shape=(64, 64, 3))\n",
    "\n",
    "\t# Conditioning Augmentation\n",
    "\tca = Dense(256)(input_layer1)\n",
    "\tmls = LeakyReLU(alpha=0.2)(ca)\n",
    "\tc = Lambda(conditioning_augmentation)(mls)\n",
    "\n",
    "\t# Downsampling block\n",
    "\tx = ZeroPadding2D(padding=(1,1))(input_images)\n",
    "\tx = Conv2D(128, kernel_size=(3,3), strides=1, use_bias=False,\n",
    "\t\t\t\tkernel_initializer='he_uniform')(x)\n",
    "\tx = ReLU()(x)\n",
    "\n",
    "\tx = ZeroPadding2D(padding=(1,1))(x)\n",
    "\tx = Conv2D(256, kernel_size=(4,4), strides=2, use_bias=False,\n",
    "\t\t\t\tkernel_initializer='he_uniform')(x)\n",
    "\tx = BatchNormalization(gamma_initializer='ones', beta_initializer='zeros')(x)\n",
    "\tx = ReLU()(x)\n",
    "\n",
    "\tx = ZeroPadding2D(padding=(1,1))(x)\n",
    "\tx = Conv2D(512, kernel_size=(4,4), strides=2, use_bias=False,\n",
    "\t\t\t\tkernel_initializer='he_uniform')(x)\n",
    "\tx = BatchNormalization(gamma_initializer='ones', beta_initializer='zeros')(x)\n",
    "\tx = ReLU()(x)\n",
    "\n",
    "\t# Concatenate text conditioning block with the encoded image\n",
    "\tconcat = concat_along_dims([c, x])\n",
    "\n",
    "\t# Residual Blocks\n",
    "\tx = ZeroPadding2D(padding=(1,1))(concat)\n",
    "\tx = Conv2D(512, kernel_size=(3,3), use_bias=False, kernel_initializer='he_uniform')(x)\n",
    "\tx = BatchNormalization(gamma_initializer='ones', beta_initializer='zeros')(x)\n",
    "\tx = ReLU()(x)\n",
    "\n",
    "\tx = residual_block(x)\n",
    "\tx = residual_block(x)\n",
    "\tx = residual_block(x)\n",
    "\tx = residual_block(x)\n",
    "\n",
    "\t# Upsampling Blocks\n",
    "\tx = UpSamplingBlock(x, 512)\n",
    "\tx = UpSamplingBlock(x, 256)\n",
    "\tx = UpSamplingBlock(x, 128)\n",
    "\tx = UpSamplingBlock(x, 64)\n",
    "\n",
    "\tx = Conv2D(3, kernel_size=(3,3), padding='same', use_bias=False, kernel_initializer='he_uniform')(x)\n",
    "\tx = Activation('tanh')(x)\n",
    "\t\n",
    "\tstage2_gen = Model(inputs=[input_layer1, input_images], outputs=[x, mls])\n",
    "\treturn stage2_gen\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_48\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_90 (InputLayer)       [(None, 64, 64, 3)]          0         []                            \n",
      "                                                                                                  \n",
      " zero_padding2d (ZeroPaddin  (None, 66, 66, 3)            0         ['input_90[0][0]']            \n",
      " g2D)                                                                                             \n",
      "                                                                                                  \n",
      " conv2d_100 (Conv2D)         (None, 64, 64, 128)          3456      ['zero_padding2d[0][0]']      \n",
      "                                                                                                  \n",
      " re_lu_59 (ReLU)             (None, 64, 64, 128)          0         ['conv2d_100[0][0]']          \n",
      "                                                                                                  \n",
      " zero_padding2d_1 (ZeroPadd  (None, 66, 66, 128)          0         ['re_lu_59[0][0]']            \n",
      " ing2D)                                                                                           \n",
      "                                                                                                  \n",
      " input_89 (InputLayer)       [(None, 1024)]               0         []                            \n",
      "                                                                                                  \n",
      " conv2d_101 (Conv2D)         (None, 32, 32, 256)          524288    ['zero_padding2d_1[0][0]']    \n",
      "                                                                                                  \n",
      " dense_48 (Dense)            (None, 256)                  262400    ['input_89[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization_80 (Ba  (None, 32, 32, 256)          1024      ['conv2d_101[0][0]']          \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " leaky_re_lu_69 (LeakyReLU)  (None, 256)                  0         ['dense_48[0][0]']            \n",
      "                                                                                                  \n",
      " re_lu_60 (ReLU)             (None, 32, 32, 256)          0         ['batch_normalization_80[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " lambda_19 (Lambda)          (None, 128)                  0         ['leaky_re_lu_69[0][0]']      \n",
      "                                                                                                  \n",
      " zero_padding2d_2 (ZeroPadd  (None, 34, 34, 256)          0         ['re_lu_60[0][0]']            \n",
      " ing2D)                                                                                           \n",
      "                                                                                                  \n",
      " tf.expand_dims (TFOpLambda  (None, 1, 128)               0         ['lambda_19[0][0]']           \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " conv2d_102 (Conv2D)         (None, 16, 16, 512)          2097152   ['zero_padding2d_2[0][0]']    \n",
      "                                                                                                  \n",
      " tf.expand_dims_1 (TFOpLamb  (None, 1, 1, 128)            0         ['tf.expand_dims[0][0]']      \n",
      " da)                                                                                              \n",
      "                                                                                                  \n",
      " batch_normalization_81 (Ba  (None, 16, 16, 512)          2048      ['conv2d_102[0][0]']          \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " tf.tile (TFOpLambda)        (None, 16, 16, 128)          0         ['tf.expand_dims_1[0][0]']    \n",
      "                                                                                                  \n",
      " re_lu_61 (ReLU)             (None, 16, 16, 512)          0         ['batch_normalization_81[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " tf.concat (TFOpLambda)      (None, 16, 16, 640)          0         ['tf.tile[0][0]',             \n",
      "                                                                     're_lu_61[0][0]']            \n",
      "                                                                                                  \n",
      " zero_padding2d_3 (ZeroPadd  (None, 18, 18, 640)          0         ['tf.concat[0][0]']           \n",
      " ing2D)                                                                                           \n",
      "                                                                                                  \n",
      " conv2d_103 (Conv2D)         (None, 16, 16, 512)          2949120   ['zero_padding2d_3[0][0]']    \n",
      "                                                                                                  \n",
      " batch_normalization_82 (Ba  (None, 16, 16, 512)          2048      ['conv2d_103[0][0]']          \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " re_lu_62 (ReLU)             (None, 16, 16, 512)          0         ['batch_normalization_82[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " conv2d_104 (Conv2D)         (None, 16, 16, 512)          2359296   ['re_lu_62[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization_83 (Ba  (None, 16, 16, 512)          2048      ['conv2d_104[0][0]']          \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " re_lu_63 (ReLU)             (None, 16, 16, 512)          0         ['batch_normalization_83[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " conv2d_105 (Conv2D)         (None, 16, 16, 512)          2359296   ['re_lu_63[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization_84 (Ba  (None, 16, 16, 512)          2048      ['conv2d_105[0][0]']          \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " add (Add)                   (None, 16, 16, 512)          0         ['batch_normalization_84[0][0]\n",
      "                                                                    ',                            \n",
      "                                                                     're_lu_62[0][0]']            \n",
      "                                                                                                  \n",
      " re_lu_64 (ReLU)             (None, 16, 16, 512)          0         ['add[0][0]']                 \n",
      "                                                                                                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " conv2d_106 (Conv2D)         (None, 16, 16, 512)          2359296   ['re_lu_64[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization_85 (Ba  (None, 16, 16, 512)          2048      ['conv2d_106[0][0]']          \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " re_lu_65 (ReLU)             (None, 16, 16, 512)          0         ['batch_normalization_85[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " conv2d_107 (Conv2D)         (None, 16, 16, 512)          2359296   ['re_lu_65[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization_86 (Ba  (None, 16, 16, 512)          2048      ['conv2d_107[0][0]']          \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " add_1 (Add)                 (None, 16, 16, 512)          0         ['batch_normalization_86[0][0]\n",
      "                                                                    ',                            \n",
      "                                                                     're_lu_64[0][0]']            \n",
      "                                                                                                  \n",
      " re_lu_66 (ReLU)             (None, 16, 16, 512)          0         ['add_1[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_108 (Conv2D)         (None, 16, 16, 512)          2359296   ['re_lu_66[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization_87 (Ba  (None, 16, 16, 512)          2048      ['conv2d_108[0][0]']          \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " re_lu_67 (ReLU)             (None, 16, 16, 512)          0         ['batch_normalization_87[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " conv2d_109 (Conv2D)         (None, 16, 16, 512)          2359296   ['re_lu_67[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization_88 (Ba  (None, 16, 16, 512)          2048      ['conv2d_109[0][0]']          \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " add_2 (Add)                 (None, 16, 16, 512)          0         ['batch_normalization_88[0][0]\n",
      "                                                                    ',                            \n",
      "                                                                     're_lu_66[0][0]']            \n",
      "                                                                                                  \n",
      " re_lu_68 (ReLU)             (None, 16, 16, 512)          0         ['add_2[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_110 (Conv2D)         (None, 16, 16, 512)          2359296   ['re_lu_68[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization_89 (Ba  (None, 16, 16, 512)          2048      ['conv2d_110[0][0]']          \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " re_lu_69 (ReLU)             (None, 16, 16, 512)          0         ['batch_normalization_89[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " conv2d_111 (Conv2D)         (None, 16, 16, 512)          2359296   ['re_lu_69[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization_90 (Ba  (None, 16, 16, 512)          2048      ['conv2d_111[0][0]']          \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " add_3 (Add)                 (None, 16, 16, 512)          0         ['batch_normalization_90[0][0]\n",
      "                                                                    ',                            \n",
      "                                                                     're_lu_68[0][0]']            \n",
      "                                                                                                  \n",
      " re_lu_70 (ReLU)             (None, 16, 16, 512)          0         ['add_3[0][0]']               \n",
      "                                                                                                  \n",
      " up_sampling2d_40 (UpSampli  (None, 32, 32, 512)          0         ['re_lu_70[0][0]']            \n",
      " ng2D)                                                                                            \n",
      "                                                                                                  \n",
      " conv2d_112 (Conv2D)         (None, 32, 32, 512)          2359296   ['up_sampling2d_40[0][0]']    \n",
      "                                                                                                  \n",
      " batch_normalization_91 (Ba  (None, 32, 32, 512)          2048      ['conv2d_112[0][0]']          \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " re_lu_71 (ReLU)             (None, 32, 32, 512)          0         ['batch_normalization_91[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " up_sampling2d_41 (UpSampli  (None, 64, 64, 512)          0         ['re_lu_71[0][0]']            \n",
      " ng2D)                                                                                            \n",
      "                                                                                                  \n",
      " conv2d_113 (Conv2D)         (None, 64, 64, 256)          1179648   ['up_sampling2d_41[0][0]']    \n",
      "                                                                                                  \n",
      " batch_normalization_92 (Ba  (None, 64, 64, 256)          1024      ['conv2d_113[0][0]']          \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " re_lu_72 (ReLU)             (None, 64, 64, 256)          0         ['batch_normalization_92[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " up_sampling2d_42 (UpSampli  (None, 128, 128, 256)        0         ['re_lu_72[0][0]']            \n",
      " ng2D)                                                                                            \n",
      "                                                                                                  \n",
      " conv2d_114 (Conv2D)         (None, 128, 128, 128)        294912    ['up_sampling2d_42[0][0]']    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                                  \n",
      " batch_normalization_93 (Ba  (None, 128, 128, 128)        512       ['conv2d_114[0][0]']          \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " re_lu_73 (ReLU)             (None, 128, 128, 128)        0         ['batch_normalization_93[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " up_sampling2d_43 (UpSampli  (None, 256, 256, 128)        0         ['re_lu_73[0][0]']            \n",
      " ng2D)                                                                                            \n",
      "                                                                                                  \n",
      " conv2d_115 (Conv2D)         (None, 256, 256, 64)         73728     ['up_sampling2d_43[0][0]']    \n",
      "                                                                                                  \n",
      " batch_normalization_94 (Ba  (None, 256, 256, 64)         256       ['conv2d_115[0][0]']          \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " re_lu_74 (ReLU)             (None, 256, 256, 64)         0         ['batch_normalization_94[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " conv2d_116 (Conv2D)         (None, 256, 256, 3)          1728      ['re_lu_74[0][0]']            \n",
      "                                                                                                  \n",
      " activation_20 (Activation)  (None, 256, 256, 3)          0         ['conv2d_116[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 28645440 (109.27 MB)\n",
      "Trainable params: 28632768 (109.23 MB)\n",
      "Non-trainable params: 12672 (49.50 KB)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "generator_stage2 = build_stage2_generator()\n",
    "generator_stage2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "############################################################\n",
    "# Stage 2 Discriminator Network\n",
    "############################################################\n",
    "\n",
    "def build_stage2_discriminator():\n",
    "\t\"\"\"Builds the Stage 2 Discriminator that uses the 256x256 resolution images from the generator\n",
    "\tand the compressed and spatially replicated embeddings.\n",
    "\n",
    "\tReturns:\n",
    "\t\tStage 2 Discriminator Model for StackGAN.\n",
    "\t\"\"\"\n",
    "\tinput_layer1 = Input(shape=(256, 256, 3))\n",
    "\n",
    "\tx = Conv2D(64, kernel_size=(4,4), padding='same', strides=2, use_bias=False,\n",
    "\t\t\t\tkernel_initializer='he_uniform')(input_layer1)\n",
    "\tx = LeakyReLU(alpha=0.2)(x)\n",
    "\n",
    "\tx = ConvBlock(x, 128)\n",
    "\tx = ConvBlock(x, 256)\n",
    "\tx = ConvBlock(x, 512)\n",
    "\tx = ConvBlock(x, 1024)\n",
    "\tx = ConvBlock(x, 2048)\n",
    "\tx = ConvBlock(x, 1024, (1,1), 1)\n",
    "\tx = ConvBlock(x, 512, (1,1), 1, False)\n",
    "\n",
    "\tx1 = ConvBlock(x, 128, (1,1), 1)\n",
    "\tx1 = ConvBlock(x1, 128, (3,3), 1)\n",
    "\tx1 = ConvBlock(x1, 512, (3,3), 1, False)\n",
    "\n",
    "\tx2 = add([x, x1])\n",
    "\tx2 = LeakyReLU(alpha=0.2)(x2)\n",
    "\n",
    "\t# Concatenate compressed and spatially replicated embedding\n",
    "\tinput_layer2 = Input(shape=(4, 4, 128))\n",
    "\tconcat = concatenate([x2, input_layer2])\n",
    "\n",
    "\tx3 = Conv2D(512, kernel_size=(1,1), strides=1, padding='same', kernel_initializer='he_uniform')(concat)\n",
    "\tx3 = BatchNormalization(gamma_initializer='ones', beta_initializer='zeros')(x3)\n",
    "\tx3 = LeakyReLU(alpha=0.2)(x3)\n",
    "\n",
    "\t# Flatten and add a FC layer\n",
    "\tx3 = Flatten()(x3)\n",
    "\tx3 = Dense(1)(x3)\n",
    "\tx3 = Activation('sigmoid')(x3)\n",
    "\n",
    "\tstage2_dis = Model(inputs=[input_layer1, input_layer2], outputs=[x3])\n",
    "\treturn stage2_dis\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_49\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_91 (InputLayer)       [(None, 256, 256, 3)]        0         []                            \n",
      "                                                                                                  \n",
      " conv2d_117 (Conv2D)         (None, 128, 128, 64)         3072      ['input_91[0][0]']            \n",
      "                                                                                                  \n",
      " leaky_re_lu_70 (LeakyReLU)  (None, 128, 128, 64)         0         ['conv2d_117[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_118 (Conv2D)         (None, 64, 64, 128)          131072    ['leaky_re_lu_70[0][0]']      \n",
      "                                                                                                  \n",
      " batch_normalization_95 (Ba  (None, 64, 64, 128)          512       ['conv2d_118[0][0]']          \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " leaky_re_lu_71 (LeakyReLU)  (None, 64, 64, 128)          0         ['batch_normalization_95[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " conv2d_119 (Conv2D)         (None, 32, 32, 256)          524288    ['leaky_re_lu_71[0][0]']      \n",
      "                                                                                                  \n",
      " batch_normalization_96 (Ba  (None, 32, 32, 256)          1024      ['conv2d_119[0][0]']          \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " leaky_re_lu_72 (LeakyReLU)  (None, 32, 32, 256)          0         ['batch_normalization_96[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " conv2d_120 (Conv2D)         (None, 16, 16, 512)          2097152   ['leaky_re_lu_72[0][0]']      \n",
      "                                                                                                  \n",
      " batch_normalization_97 (Ba  (None, 16, 16, 512)          2048      ['conv2d_120[0][0]']          \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " leaky_re_lu_73 (LeakyReLU)  (None, 16, 16, 512)          0         ['batch_normalization_97[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " conv2d_121 (Conv2D)         (None, 8, 8, 1024)           8388608   ['leaky_re_lu_73[0][0]']      \n",
      "                                                                                                  \n",
      " batch_normalization_98 (Ba  (None, 8, 8, 1024)           4096      ['conv2d_121[0][0]']          \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " leaky_re_lu_74 (LeakyReLU)  (None, 8, 8, 1024)           0         ['batch_normalization_98[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " conv2d_122 (Conv2D)         (None, 4, 4, 2048)           3355443   ['leaky_re_lu_74[0][0]']      \n",
      "                                                          2                                       \n",
      "                                                                                                  \n",
      " batch_normalization_99 (Ba  (None, 4, 4, 2048)           8192      ['conv2d_122[0][0]']          \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " leaky_re_lu_75 (LeakyReLU)  (None, 4, 4, 2048)           0         ['batch_normalization_99[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " conv2d_123 (Conv2D)         (None, 4, 4, 1024)           2097152   ['leaky_re_lu_75[0][0]']      \n",
      "                                                                                                  \n",
      " batch_normalization_100 (B  (None, 4, 4, 1024)           4096      ['conv2d_123[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " leaky_re_lu_76 (LeakyReLU)  (None, 4, 4, 1024)           0         ['batch_normalization_100[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " conv2d_124 (Conv2D)         (None, 4, 4, 512)            524288    ['leaky_re_lu_76[0][0]']      \n",
      "                                                                                                  \n",
      " batch_normalization_101 (B  (None, 4, 4, 512)            2048      ['conv2d_124[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " conv2d_125 (Conv2D)         (None, 4, 4, 128)            65536     ['batch_normalization_101[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " batch_normalization_102 (B  (None, 4, 4, 128)            512       ['conv2d_125[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " leaky_re_lu_77 (LeakyReLU)  (None, 4, 4, 128)            0         ['batch_normalization_102[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " conv2d_126 (Conv2D)         (None, 4, 4, 128)            147456    ['leaky_re_lu_77[0][0]']      \n",
      "                                                                                                  \n",
      " batch_normalization_103 (B  (None, 4, 4, 128)            512       ['conv2d_126[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " leaky_re_lu_78 (LeakyReLU)  (None, 4, 4, 128)            0         ['batch_normalization_103[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " conv2d_127 (Conv2D)         (None, 4, 4, 512)            589824    ['leaky_re_lu_78[0][0]']      \n",
      "                                                                                                  \n",
      " batch_normalization_104 (B  (None, 4, 4, 512)            2048      ['conv2d_127[0][0]']          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " add_4 (Add)                 (None, 4, 4, 512)            0         ['batch_normalization_101[0][0\n",
      "                                                                    ]',                           \n",
      "                                                                     'batch_normalization_104[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " leaky_re_lu_79 (LeakyReLU)  (None, 4, 4, 512)            0         ['add_4[0][0]']               \n",
      "                                                                                                  \n",
      " input_92 (InputLayer)       [(None, 4, 4, 128)]          0         []                            \n",
      "                                                                                                  \n",
      " concatenate_20 (Concatenat  (None, 4, 4, 640)            0         ['leaky_re_lu_79[0][0]',      \n",
      " e)                                                                  'input_92[0][0]']            \n",
      "                                                                                                  \n",
      " conv2d_128 (Conv2D)         (None, 4, 4, 512)            328192    ['concatenate_20[0][0]']      \n",
      "                                                                                                  \n",
      " batch_normalization_105 (B  (None, 4, 4, 512)            2048      ['conv2d_128[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " leaky_re_lu_80 (LeakyReLU)  (None, 4, 4, 512)            0         ['batch_normalization_105[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " flatten_10 (Flatten)        (None, 8192)                 0         ['leaky_re_lu_80[0][0]']      \n",
      "                                                                                                  \n",
      " dense_49 (Dense)            (None, 1)                    8193      ['flatten_10[0][0]']          \n",
      "                                                                                                  \n",
      " activation_21 (Activation)  (None, 1)                    0         ['dense_49[0][0]']            \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 48486401 (184.96 MB)\n",
      "Trainable params: 48472833 (184.91 MB)\n",
      "Non-trainable params: 13568 (53.00 KB)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "discriminator_stage2 = build_stage2_discriminator()\n",
    "discriminator_stage2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "############################################################\n",
    "# Stage 2 Adversarial Model\n",
    "############################################################\n",
    "\n",
    "def stage2_adversarial_network(stage2_disc, stage2_gen, stage1_gen):\n",
    "\t\"\"\"Stage 2 Adversarial Network.\n",
    "\n",
    "\tArgs:\n",
    "\t\tstage2_disc: Stage 2 Discriminator Model.\n",
    "\t\tstage2_gen: Stage 2 Generator Model.\n",
    "\t\tstage1_gen: Stage 1 Generator Model.\n",
    "\n",
    "\tReturns:\n",
    "\t\tStage 2 Adversarial network.\n",
    "\t\"\"\"\n",
    "\tconditioned_embedding = Input(shape=(1024, ))\n",
    "\tlatent_space = Input(shape=(100, ))\n",
    "\tcompressed_replicated = Input(shape=(4, 4, 128))\n",
    "    \n",
    "\t#the discriminator is trained separately and stage1_gen already trained, and this is the reason why we freeze its layers by setting the property trainable=false\n",
    "\tinput_images, ca = stage1_gen([conditioned_embedding, latent_space])\n",
    "\tstage2_disc.trainable = False\n",
    "\tstage1_gen.trainable = False\n",
    "\n",
    "\timages, ca2 = stage2_gen([conditioned_embedding, input_images])\n",
    "\tprobability = stage2_disc([images, compressed_replicated])\n",
    "\n",
    "\treturn Model(inputs=[conditioned_embedding, latent_space, compressed_replicated],\n",
    "\t\toutputs=[probability, ca2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_50\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_93 (InputLayer)       [(None, 1024)]               0         []                            \n",
      "                                                                                                  \n",
      " input_94 (InputLayer)       [(None, 100)]                0         []                            \n",
      "                                                                                                  \n",
      " model (Functional)          [(None, 64, 64, 3),          1027040   ['input_93[0][0]',            \n",
      "                              (None, 256)]                0          'input_94[0][0]']            \n",
      "                                                                                                  \n",
      " model_48 (Functional)       [(None, 256, 256, 3),        2864544   ['input_93[0][0]',            \n",
      "                              (None, 256)]                0          'model[1][0]']               \n",
      "                                                                                                  \n",
      " input_95 (InputLayer)       [(None, 4, 4, 128)]          0         []                            \n",
      "                                                                                                  \n",
      " model_49 (Functional)       (None, 1)                    4848640   ['model_48[0][0]',            \n",
      "                                                          1          'input_95[0][0]']            \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 87402241 (333.41 MB)\n",
      "Trainable params: 28632768 (109.23 MB)\n",
      "Non-trainable params: 58769473 (224.19 MB)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "adversarial_stage2 = stage2_adversarial_network(discriminator_stage2, generator_stage2, generator)\n",
    "adversarial_stage2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = r'C:/Users/purpl/Downloads/text_to_image_GAN/birds'\n",
    "train_dir = data_dir + \"/train\"\n",
    "test_dir = data_dir + \"/test\"\n",
    "embeddings_path_train = train_dir + \"/char-CNN-RNN-embeddings.pickle\"\n",
    "embeddings_path_test = test_dir + \"/char-CNN-RNN-embeddings.pickle\"\n",
    "filename_path_train = train_dir + \"/filenames.pickle\"\n",
    "filename_path_test = test_dir + \"/filenames.pickle\"\n",
    "class_id_path_train = train_dir + \"/class_info.pickle\"\n",
    "class_id_path_test = test_dir + \"/class_info.pickle\"\n",
    "dataset_path = r'C:/Users/purpl/Downloads/text_to_image_GAN/CUB_200_2011/CUB_200_2011'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class StackGanStage2(object):\n",
    "\t\"\"\"StackGAN Stage 2 class.\n",
    "\n",
    "\tArgs:\n",
    "\t\tepochs: Number of epochs\n",
    "\t\tz_dim: Latent space dimensions\n",
    "\t\tbatch_size: Batch Size\n",
    "\t\tenable_function: If True, training function is decorated with tf.function\n",
    "\t\tstage2_generator_lr: Learning rate for stage 2 generator\n",
    "\t\tstage2_discriminator_lr: Learning rate for stage 2 discriminator\n",
    "\t\"\"\"\n",
    "\tdef __init__(self, epochs=1, z_dim=100, batch_size=64, enable_function=True, stage2_generator_lr=0.0002, stage2_discriminator_lr=0.0002):\n",
    "\t\tself.epochs = epochs\n",
    "\t\tself.z_dim = z_dim\n",
    "\t\tself.enable_function = enable_function\n",
    "\t\tself.stage1_generator_lr = stage2_generator_lr\n",
    "\t\tself.stage1_discriminator_lr = stage2_discriminator_lr\n",
    "\t\tself.low_image_size = 64\n",
    "\t\tself.high_image_size = 256\n",
    "\t\tself.conditioning_dim = 128\n",
    "\t\tself.batch_size = batch_size\n",
    "\t\tself.stage2_generator_optimizer = Adam(lr=stage2_generator_lr, beta_1=0.5, beta_2=0.999)\n",
    "\t\tself.stage2_discriminator_optimizer = Adam(lr=stage2_discriminator_lr, beta_1=0.5, beta_2=0.999)\n",
    "\t\tself.stage1_generator = build_stage1_generator()\n",
    "\t\tself.stage1_generator.compile(loss='binary_crossentropy', optimizer=self.stage2_generator_optimizer)\n",
    "\t\tself.stage1_generator.load_weights('weights/stage1_gen.h5')\n",
    "\t\tself.stage2_generator = build_stage2_generator()\n",
    "\t\tself.stage2_generator.compile(loss='binary_crossentropy', optimizer=self.stage2_generator_optimizer)\n",
    "\n",
    "\t\tself.stage2_discriminator = build_stage2_discriminator()\n",
    "\t\tself.stage2_discriminator.compile(loss='binary_crossentropy', optimizer=self.stage2_discriminator_optimizer)\n",
    "\n",
    "\t\tself.ca_network = build_ca_network()\n",
    "\t\tself.ca_network.compile(loss='binary_crossentropy', optimizer='Adam')\n",
    "\n",
    "\t\tself.embedding_compressor = build_embedding_compressor()\n",
    "\t\tself.embedding_compressor.compile(loss='binary_crossentropy', optimizer='Adam')\n",
    "\n",
    "\t\tself.stage2_adversarial = stage2_adversarial_network(self.stage2_discriminator, self.stage2_generator, self.stage1_generator)\n",
    "\t\tself.stage2_adversarial.compile(loss=['binary_crossentropy', adversarial_loss], loss_weights=[1, 2.0], optimizer=self.stage2_generator_optimizer)\t\n",
    "\n",
    "\t\tself.checkpoint2 = tf.train.Checkpoint(\n",
    "        \tgenerator_optimizer=self.stage2_generator_optimizer,\n",
    "        \tdiscriminator_optimizer=self.stage2_discriminator_optimizer,\n",
    "        \tgenerator=self.stage2_generator,\n",
    "        \tdiscriminator=self.stage2_discriminator,\n",
    "        \tgenerator1=self.stage1_generator)\n",
    "\n",
    "\tdef visualize_stage2(self):\n",
    "\t\t\"\"\"Running Tensorboard visualizations.\n",
    "\t\t\"\"\"\n",
    "\t\ttb = TensorBoard(log_dir=\"logs/\".format(time.time()))\n",
    "\t\ttb.set_model(self.stage2_generator)\n",
    "\t\ttb.set_model(self.stage2_discriminator)\n",
    "\n",
    "\tdef train_stage2(self):\n",
    "\t\t\"\"\"Trains Stage 2 StackGAN.\n",
    "\t\t\"\"\"\n",
    "\t\tx_high_train, y_high_train, high_train_embeds = load_data(filename_path=filename_path_train, class_id_path=class_id_path_train,\n",
    "      dataset_path=dataset_path, embeddings_path=embeddings_path_train, size=(256, 256))\n",
    "\n",
    "\t\tx_high_test, y_high_test, high_test_embeds = load_data(filename_path=filename_path_test, class_id_path=class_id_path_test, \n",
    "      dataset_path=dataset_path, embeddings_path=embeddings_path_test, size=(256, 256))\n",
    "\n",
    "\t\tx_low_train, y_low_train, low_train_embeds = load_data(filename_path=filename_path_train, class_id_path=class_id_path_train,\n",
    "      dataset_path=dataset_path, embeddings_path=embeddings_path_train, size=(64, 64))\n",
    "\n",
    "\t\tx_low_test, y_low_test, low_test_embeds = load_data(filename_path=filename_path_test, class_id_path=class_id_path_test, \n",
    "      dataset_path=dataset_path, embeddings_path=embeddings_path_test, size=(64, 64))\n",
    "\n",
    "\t\treal = np.ones((self.batch_size, 1), dtype='float') * 0.9\n",
    "\t\tfake = np.zeros((self.batch_size, 1), dtype='float') * 0.1\n",
    "\n",
    "\t\tfor epoch in range(self.epochs):\n",
    "\t\t\tprint(f'Epoch: {epoch}')\n",
    "\n",
    "\t\t\tgen_loss = []\n",
    "\t\t\tdisc_loss = []\n",
    "\n",
    "\t\t\tnum_batches = int(x_high_train.shape[0] / self.batch_size)\n",
    "\n",
    "\t\t\tfor i in range(num_batches):\n",
    "\n",
    "\t\t\t\tlatent_space = np.random.normal(0, 1, size=(self.batch_size, self.z_dim))\n",
    "\t\t\t\tembedding_text = high_train_embeds[i * self.batch_size:(i + 1) * self.batch_size]\n",
    "\t\t\t\tcompressed_embedding = self.embedding_compressor.predict_on_batch(embedding_text)\n",
    "\t\t\t\tcompressed_embedding = np.reshape(compressed_embedding, (-1, 1, 1, self.conditioning_dim))\n",
    "\t\t\t\tcompressed_embedding = np.tile(compressed_embedding, (1, 4, 4, 1))\n",
    "\n",
    "\t\t\t\timage_batch = x_high_train[i * self.batch_size:(i+1) * self.batch_size]\n",
    "\t\t\t\timage_batch = (image_batch - 127.5) / 127.5\n",
    "\t\t\t\t\n",
    "\t\t\t\tlow_res_fakes, _ = self.stage1_generator.predict([embedding_text, latent_space], verbose=3)\n",
    "\t\t\t\thigh_res_fakes, _ = self.stage2_generator.predict([embedding_text, low_res_fakes], verbose=3)\n",
    "\n",
    "\t\t\t\tdiscriminator_loss = self.stage2_discriminator.train_on_batch([image_batch, compressed_embedding],\n",
    "\t\t\t\t\tnp.reshape(real, (self.batch_size, 1)))\n",
    "\n",
    "\t\t\t\tdiscriminator_loss_gen = self.stage2_discriminator.train_on_batch([high_res_fakes, compressed_embedding],\n",
    "\t\t\t\t\tnp.reshape(fake, (self.batch_size, 1)))\n",
    "\n",
    "\t\t\t\tdiscriminator_loss_fake = self.stage2_discriminator.train_on_batch([image_batch[:(self.batch_size-1)], compressed_embedding[1:]],\n",
    "\t\t\t\t\tnp.reshape(fake[1:], (self.batch_size - 1, 1)))\n",
    "\n",
    "\t\t\t\td_loss = 0.5 * np.add(discriminator_loss, 0.5 * np.add(discriminator_loss_gen, discriminator_loss_fake))\n",
    "\t\t\t\tdisc_loss.append(d_loss)\n",
    "\n",
    "\t\t\t\tprint(f'Discriminator Loss: {d_loss}')\n",
    "\n",
    "\t\t\t\tg_loss = self.stage2_adversarial.train_on_batch([embedding_text, latent_space, compressed_embedding],\n",
    "\t\t\t\t\t[K.ones((self.batch_size, 1)) * 0.9, K.ones((self.batch_size, 256)) * 0.9])\n",
    "\t\t\t\tgen_loss.append(g_loss)\n",
    "\n",
    "\t\t\t\tprint(f'Generator Loss: {g_loss}')\n",
    "\n",
    "\t\t\t\tif epoch % 5 == 0:\n",
    "\t\t\t\t\tlatent_space = np.random.normal(0, 1, size=(self.batch_size, self.z_dim))\n",
    "\t\t\t\t\tembedding_batch = high_test_embeds[0 : self.batch_size]\n",
    "\n",
    "\t\t\t\t\tlow_fake_images, _ = self.stage1_generator.predict([embedding_batch, latent_space], verbose=3)\n",
    "\t\t\t\t\thigh_fake_images, _ = self.stage2_generator.predict([embedding_batch, low_fake_images], verbose=3)\n",
    "\n",
    "\t\t\t\t\tfor i, image in enumerate(high_fake_images[:10]):\n",
    "\t\t\t\t\t    save_image(image, f'results_stage2/gen_{epoch}_{i}.png')\n",
    "\n",
    "\t\t\t\tif epoch % 10 == 0:\n",
    "\t\t\t\t\tself.stage2_generator.save_weights('weights/stage2_gen.h5')\n",
    "\t\t\t\t\tself.stage2_discriminator.save_weights(\"weights/stage2_disc.h5\")\n",
    "\t\t\t\t\tself.ca_network.save_weights('weights/stage2_ca.h5')\n",
    "\t\t\t\t\tself.embedding_compressor.save_weights('weights/stage2_embco.h5')\n",
    "\t\t\t\t\tself.stage2_adversarial.save_weights('weights/stage2_adv.h5')\n",
    "\n",
    "\t\tself.stage2_generator.save_weights('weights/stage2_gen.h5')\n",
    "\t\tself.stage2_discriminator.save_weights(\"weights/stage2_disc.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "Discriminator Loss: 5.668106436729431\n"
     ]
    }
   ],
   "source": [
    "stage2 = StackGanStage2()\n",
    "stage2.train_stage2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "stackgan.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
